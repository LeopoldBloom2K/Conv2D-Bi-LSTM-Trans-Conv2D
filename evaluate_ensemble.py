import torch
import soundfile as sf
import numpy as np
import argparse
import os
import sys
import museval
from tqdm import tqdm
import pandas as pd

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models.crnn_separator import CRNN_Separator
from utils.audio_processor import AudioProcessor

def evaluate_ensemble(args):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ðŸš€ Ensemble Evaluation Device: {device}")
    
    SR = 22050
    N_FFT = 1024
    HOP_LENGTH = 256
    N_BINS = N_FFT // 2 
    
    # 1. ë‘ ê°œì˜ ëª¨ë¸ ë¡œë“œ
    def load_weights(model, path, device):
        checkpoint = torch.load(path, map_location=device)
        # í‚¤ê°€ 'model_state_dict' ë‚´ë¶€ì— ìžˆëŠ”ì§€, ì•„ë‹ˆë©´ ë°ì´í„° ê·¸ ìžì²´ì¸ì§€ í™•ì¸
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        model.eval()
        return model

    print(f"ðŸ“¦ Loading Model 1: {args.model_path1}")
    model1 = CRNN_Separator(input_channels=2, n_bins=N_BINS, num_stems=4, 
                            hidden_size=args.hidden_size, num_layers=args.num_layers).to(device)
    model1 = load_weights(model1, args.model_path1, device)

    print(f"ðŸ“¦ Loading Model 2: {args.model_path2}")
    model2 = CRNN_Separator(input_channels=2, n_bins=N_BINS, num_stems=4, 
                            hidden_size=args.hidden_size, num_layers=args.num_layers).to(device)
    model2 = load_weights(model2, args.model_path2, device)
    
    processor = AudioProcessor(sr=SR, n_fft=N_FFT, hop_length=HOP_LENGTH)
    
    # 2. ê²½ë¡œ í™•ì¸
    test_path = os.path.abspath(args.test_dir)
    if os.path.isfile(test_path):
        test_path = os.path.dirname(test_path)
    
    if os.path.exists(os.path.join(test_path, "mixture.wav")):
        song_folders = [test_path]
    else:
        song_folders = [f.path for f in os.scandir(test_path) if f.is_dir()]
    
    print(f"ðŸ” ì´ {len(song_folders)}ê°œì˜ ê³¡ì„ ì•™ìƒë¸” í‰ê°€í•©ë‹ˆë‹¤.")
    
    sdr_results = []
    
    for song_dir in tqdm(song_folders):
        try:
            song_name = os.path.basename(song_dir)
            mix_path = os.path.join(song_dir, "mixture.wav")
            target_path = os.path.join(song_dir, f"{args.target}.wav")
            
            if not os.path.exists(mix_path) or not os.path.exists(target_path):
                continue

            mix_audio = processor.load_audio(mix_path)
            ref_audio = processor.load_audio(target_path)
            
            min_len = min(mix_audio.shape[1], ref_audio.shape[1])
            mix_audio = mix_audio[:, :min_len]
            ref_audio = ref_audio[:, :min_len]

            mix_mag, mix_phase = processor.audio_to_stft(mix_audio)
            mix_mag_tensor = mix_mag.unsqueeze(0).to(device)

            with torch.no_grad():
                # ë‘ ëª¨ë¸ì˜ ë§ˆìŠ¤í¬ë¥¼ ê°ê° ì˜ˆì¸¡
                mask1 = model1(mix_mag_tensor)
                mask2 = model2(mix_mag_tensor)
                
                # ì•™ìƒë¸”: ë‘ ë§ˆìŠ¤í¬ì˜ ì‚°ìˆ  í‰ê·  (ê°€ìž¥ ì•ˆì •ì ì¸ ë°©ì‹)
                # ê°€ì¤‘ì¹˜ë¥¼ ì£¼ê³  ì‹¶ë‹¤ë©´ (mask1 * 0.4 + mask2 * 0.6) ì‹ìœ¼ë¡œ ì¡°ì ˆ ê°€ëŠ¥
                ensemble_mask = (mask1 + mask2) / 2.0
                
                # ì˜µì…˜ ì ìš© (ì•™ìƒë¸” ì‹œì—ëŠ” ìˆœì • 1.0/0.1 ì¶”ì²œ)
                if args.mask_scale != 1.0:
                    ensemble_mask = ensemble_mask ** args.mask_scale
                
                masks_np = ensemble_mask.squeeze(0).cpu().numpy()

                if args.threshold > 0.0:
                    masks_np[masks_np < args.threshold] = 0.0

            stem_indices = {'vocals': 0, 'drums': 1, 'bass': 2, 'other': 3}
            mask = masks_np[stem_indices.get(args.target, 0)]

            # ë³µì› ë° SDR ê³„ì‚°
            est_mag = mix_mag.cpu().numpy() * mask
            est_audio = processor.stft_to_audio(est_mag, mix_phase)

            ref = ref_audio.T[None, :, :]
            est = est_audio.T[None, :, :]
            L = min(ref.shape[1], est.shape[1])
            sdr, _, _, _ = museval.evaluate(ref[:, :L, :], est[:, :L, :], win=L, hop=L)
            
            sdr_results.append({'song': song_name, 'sdr': np.nanmedian(sdr)})
            
        except Exception as e:
            print(f"Error: {e}")
            continue

    df = pd.DataFrame(sdr_results)
    print("\n" + "="*40)
    print(f"ðŸ† ì•™ìƒë¸” ê²°ê³¼ ({args.target})")
    print(f"   - í‰ê·  SDR: {df['sdr'].mean():.4f} dB")
    print("="*40)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--test_dir', type=str, required=True)
    parser.add_argument('--target', type=str, default='vocals')
    # ë‘ ê°œì˜ ëª¨ë¸ ê²½ë¡œë¥¼ ë°›ìŒ
    parser.add_argument('--model_path1', type=str, required=True, help='Large V1 (3.79dB)')
    parser.add_argument('--model_path2', type=str, required=True, help='Final Polish (3.93dB)')
    
    parser.add_argument('--hidden_size', type=int, default=512)
    parser.add_argument('--num_layers', type=int, default=4)
    parser.add_argument('--mask_scale', type=float, default=1.0)
    parser.add_argument('--threshold', type=float, default=0.1)
    
    args = parser.parse_args()
    evaluate_ensemble(args)